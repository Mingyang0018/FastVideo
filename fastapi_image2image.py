from fastapi import FastAPI
from pydantic import BaseModel
from diffusers import DiffusionPipeline
import torch
import gc
from typing import Optional
from PIL import Image
import io
import base64
import os
import sys
import signal

app = FastAPI(title="image2image API")

# ------------------- è¾“å…¥æ•°æ®ç»“æ„ -------------------
class T2IRequest(BaseModel):
    prompt: str
    negative_prompt: Optional[str] = ""
    image_base64: str
    width: int = 512
    height: int = 512
    num_inference_steps: int = 25

class ModelRequest(BaseModel):
    model_id: str

pipe: DiffusionPipeline = None

@app.post("/load_model")
def load_model(req: ModelRequest):
    global pipe
    if pipe is not None:
        del pipe
        gc.collect()
        torch.cuda.empty_cache()
        print("â™»ï¸ æ—§æ¨¡å‹å·²å¸è½½")

    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {req.model_id}")
    try:
        cuda_count = torch.cuda.device_count()
        # No GPU -> CPU
        if cuda_count == 0:
            print("â„¹ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU åŠ è½½æ¨¡å‹")
            pipe = DiffusionPipeline.from_pretrained(
                req.model_id,
                local_files_only=True,
            )
            pipe.to("cpu")

        # Single GPU -> load fp16 to single card
        elif cuda_count == 1:
            print("â„¹ï¸ æ£€æµ‹åˆ° 1 ä¸ª GPUï¼Œä½¿ç”¨å•å¡ï¼ˆcuda:0ï¼‰ï¼Œé‡‡ç”¨ float16 å‡å°‘æ˜¾å­˜å ç”¨")
            pipe = DiffusionPipeline.from_pretrained(
                req.model_id,
                # torch_dtype=torch.float16,
                # low_cpu_mem_usage=True,
                device_map="balanced",
                local_files_only=True,
            )
            # pipe.to("cuda")

        # Multi GPU -> try to shard/parallelize across cards
        else:
            print(f"â„¹ï¸ æ£€æµ‹åˆ° {cuda_count} ä¸ª GPUï¼Œå°è¯•å°†æ¨¡å‹åˆ†é…åˆ°å¤šå¡")
            # prefer device_map="auto" when available, fallback to "balanced"
            tried_auto = False
            try:
                pipe = DiffusionPipeline.from_pretrained(
                    req.model_id,
                    # torch_dtype=torch.float16,
                    device_map="auto",
                    # low_cpu_mem_usage=True,
                    local_files_only=True,
                )
                tried_auto = True
                print("â„¹ï¸ ä½¿ç”¨ device_map='auto' è‡ªåŠ¨åˆ†é…åˆ°å¤šå¡")
            except Exception:
                print("âš ï¸ device_map='auto' ä¸å¯ç”¨ï¼Œé€€å› device_map='balanced'")
                pipe = DiffusionPipeline.from_pretrained(
                    req.model_id,
                    # torch_dtype=torch.float16,
                    device_map="balanced",
                    # low_cpu_mem_usage=True,
                    local_files_only=True,
                )

            # parallelize will place modules according to device_map
            try:
                pipe.parallelize()
            except Exception:
                # å¦‚æœ parallelize ä¸å¯ç”¨ï¼Œå°è¯• enable_model_cpu_offloadï¼ˆå…¼å®¹ä¸åŒ diffusers ç‰ˆæœ¬ï¼‰
                try:
                    pipe.enable_model_cpu_offload()
                except Exception:
                    pass

        # common optimizations
        try:
            pipe.enable_attention_slicing()
        except Exception:
            pass
        try:
            pipe.enable_vae_slicing()
        except Exception:
            pass

        print(f"âœ… æ¨¡å‹ {req.model_id} åŠ è½½å®Œæˆ")
        return {"status": "ok", "model_id": req.model_id}
    except Exception as e:
        import traceback
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥:", e)
        traceback.print_exc()
        return {"status": "error", "message": str(e)}

# @app.post("/load_model")
# def load_model(req: ModelRequest):
#     global pipe
#     if pipe is not None:
#         del pipe
#         gc.collect()
#         torch.cuda.empty_cache()
#         print("â™»ï¸ æ—§æ¨¡å‹å·²å¸è½½")

#     print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {req.model_id}")
#     try:
#         # æ ¹æ®ç³»ç»Ÿ GPU æ•°é‡è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ç­–ç•¥ï¼š
#         # - 0 GPU: ä½¿ç”¨ CPU
#         # - 1 GPU: å°†æ•´ä¸ª pipeline æ”¾åˆ°å•ä¸ª GPUï¼ˆç²¾åº¦ä½¿ç”¨ float16 ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
#         # - å¤š GPU: ä½¿ç”¨ device_map="balanced" è®© accelerate/transformers åœ¨å¤šå¡ä¸Šåˆ†é…
#         cuda_count = torch.cuda.device_count()
#         if cuda_count == 0:
#             print("â„¹ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU åŠ è½½æ¨¡å‹")
#             pipe = DiffusionPipeline.from_pretrained(
#                 req.model_id,
#                 local_files_only=True,
#             )
#             pipe.to("cpu")
#         elif cuda_count == 1:
#             print("â„¹ï¸ æ£€æµ‹åˆ° 1 ä¸ª GPUï¼Œä½¿ç”¨å•å¡ï¼ˆcuda:0ï¼‰")
#             pipe = DiffusionPipeline.from_pretrained(
#                 req.model_id,
#                 # torch_dtype=torch.float16,
#                 local_files_only=True,
#             )
#             pipe.to("cuda")
#         else:
#             print(f"â„¹ï¸ æ£€æµ‹åˆ° {cuda_count} ä¸ª GPUï¼Œä½¿ç”¨ device_map='balanced' åˆ†é…åˆ°å¤šå¡")
#             pipe = DiffusionPipeline.from_pretrained(
#                 req.model_id,
#                 device_map="balanced",
#                 # torch_dtype=torch.float16,
#                 local_files_only=True,
#             )
#             pipe.parallelize()

#         pipe.enable_attention_slicing()
#         print(f"âœ… æ¨¡å‹ {req.model_id} åŠ è½½å®Œæˆ")
#         return {"status": "ok", "model_id": req.model_id}
#     except Exception as e:
#         import traceback
#         print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥:", e)
#         traceback.print_exc()
#         return {"status": "error", "message": str(e)}

@app.post("/image2image")
def image2image(req: T2IRequest):
    global pipe
    if pipe is None:
        return {"error": "æ¨¡å‹æœªåŠ è½½"}
    
    app.state.busy = True
    try:
        image_bytes = base64.b64decode(req.image_base64)
        init_image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        
        image = pipe(
            prompt=req.prompt,
            negative_prompt=req.negative_prompt,
            image=init_image,
            num_images_per_prompt=1,
            width=req.width,
            height=req.height,
            num_inference_steps=req.num_inference_steps
        ).images[0]
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
        return {"image_base64": img_str}
    except Exception as e:
        # æ•è·å¼‚å¸¸å¹¶è¿”å›
        return {"error": str(e)}

    finally:
        # é‡Šæ”¾çŠ¶æ€ & æ¸…ç†èµ„æº
        app.state.busy = False
        for var in ["image", "init_image", "buffered", "image_bytes"]:
            if var in locals():
                del locals()[var]
        gc.collect()
        torch.cuda.empty_cache()

@app.post("/shutdown")
def shutdown_server():
    """
    è¿œç¨‹å…³é—­ uvicorn æœåŠ¡ + é‡Šæ”¾æ˜¾å­˜
    """
    global pipe
    if pipe is not None:
        del pipe
        gc.collect()
        torch.cuda.empty_cache()
        print("âœ… æ¨¡å‹å·²é‡Šæ”¾ï¼Œæ˜¾å­˜å›æ”¶å®Œæˆ")

    print("ğŸ›‘ æ”¶åˆ°å…³é—­è¯·æ±‚ï¼Œæ­£åœ¨é€€å‡º FastAPI...")
    # å½»åº•é€€å‡º uvicorn
    os.kill(os.getpid(), signal.SIGTERM)
    return {"status": "shutting down"}

@app.get("/busy")
def busy_status():
    return {"busy": getattr(app.state, "busy", False)}

@app.get("/health")
def health_check():
    return {"status": "ok"}