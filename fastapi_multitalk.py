import subprocess
import os
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, List
import tempfile
import json
import librosa  # ç”¨äºè¯»å–éŸ³é¢‘æ—¶é•¿
import soundfile as sf
import numpy as np
import gc
import torch
import signal

app = FastAPI(title="MultiTalk API", description="FastAPI wrapper for MultiTalk video generation")

class MultiTalkRequest(BaseModel):
    ckpt_dir: str = "weights/Wan2.1-I2V-14B-480P"
    wav2vec_dir: str = "weights/chinese-wav2vec2-base"
    input_json: str = "examples/single_example_1.json"
    lora_dir: Optional[str] = "weights/Wan2.1_I2V_14B_FusionX_LoRA.safetensors"
    lora_scale: Optional[float] = 1.2
    sample_text_guide_scale: float = 1.0
    sample_audio_guide_scale: float = 2.0
    sample_steps: int = 8
    mode: str = "streaming"
    num_persistent_param_in_dit: int = 0
    save_file: str = "single_long_fusionx_exp"
    sample_shift: int = 2
    # quant_dir: str = "weights/MeiGen-MultiTalk"
    
def pad_audio(audio_path: str, min_duration: float = 82/25) -> str:
    """è¡¥é½éŸ³é¢‘åˆ° min_duration ç§’ï¼Œè¿”å›æ–°éŸ³é¢‘è·¯å¾„"""
    try:
        # è¯»å–éŸ³é¢‘
        y, sr = librosa.load(audio_path, sr=None)
        duration = librosa.get_duration(y=y, sr=sr)
        print('duration: ', duration)
        if duration >= min_duration:
            return audio_path  # ä¸éœ€è¦è¡¥é½
        # åœ¨å°¾éƒ¨è¿½åŠ æ¸å¼±å°¾éŸ³
        fade_len = int(0.01 * sr)  # 10ms
        if fade_len < len(y):
            fade_out = np.linspace(1.0, 0.0, fade_len)
            y = np.concatenate([y, y[-fade_len:] * fade_out])

        # è¡¥é½å‰©ä½™é™éŸ³
        duration_after_fade = len(y) / sr
        pad_len = int(max(0, (min_duration - duration_after_fade) * sr))
        if pad_len > 0:
            y = np.concatenate([y, np.zeros((pad_len,))])

        # è¦†ç›–åŸå§‹æ–‡ä»¶
        sf.write(audio_path, y, sr)
        return audio_path

    except Exception as e:
        print(f"âš ï¸ éŸ³é¢‘è¡¥é½å¤±è´¥: {e}")
        return audio_path


@app.post("/generate")
def generate_video(req: MultiTalkRequest, stream_output=True):
    """
    Run MultiTalk pipeline to generate lip-sync video.
    """
    # è¯»å–å¹¶ä¿®æ”¹ input_jsonï¼Œå¤„ç†çŸ­éŸ³é¢‘
    with open(req.input_json, "r", encoding="utf-8") as f:
        data = json.load(f)

    cond_audio = data.get("cond_audio", {})
    for k, v in cond_audio.items():
        if os.path.exists(v):
            pad_audio(v, min_duration=82/25)  # è‡ªåŠ¨è¡¥é½çŸ­éŸ³é¢‘

    temp_audio_dir = tempfile.mkdtemp()  # ç³»ç»Ÿä¸´æ—¶ç›®å½•

    cmd = [
        "python", "generate_multitalk.py",
        "--ckpt_dir", req.ckpt_dir,
        "--wav2vec_dir", req.wav2vec_dir,
        "--input_json", req.input_json,
        "--sample_text_guide_scale", str(req.sample_text_guide_scale),
        "--sample_audio_guide_scale", str(req.sample_audio_guide_scale),
        "--sample_steps", str(req.sample_steps),
        "--mode", req.mode,
        "--num_persistent_param_in_dit", str(req.num_persistent_param_in_dit),
        "--use_teacache", 
        "--teacache_thresh", "0.5", 
        "--save_file", req.save_file,
        "--sample_shift", str(req.sample_shift),
        "--audio_save_dir", temp_audio_dir,
        # "--quant", "int8",
        # "--quant_dir", req.quant_dir
    ]

    # only add LoRA if provided
    if req.lora_dir:
        cmd += ["--lora_dir", req.lora_dir, "--lora_scale", str(req.lora_scale)]

    # if req.quant_dir:
    #     cmd += ["--quant", "int8", "--quant_dir", req.quant_dir]
    # cuda_count = torch.cuda.device_count()

    # ç”Ÿæˆå™¨å‡½æ•°ï¼Œé€è¡Œè¯»å–å­è¿›ç¨‹è¾“å‡º
    def run_and_stream():
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
        )

        line_count = 0

        yield f"\n cmd: {cmd}"
        for line in iter(process.stdout.readline, ''):
            if line_count % 10 == 0:
                yield line  # ğŸš€ é€è¡Œè¿”å›ç»™å®¢æˆ·ç«¯
            line_count += 1
        process.wait()
        yield f"\n[ç»“æŸ] returncode={process.returncode}\n"
        video_path = f"{req.save_file}.mp4"
        if os.path.exists(video_path):
            yield f"[æˆåŠŸ] è§†é¢‘å·²ç”Ÿæˆ: {video_path}\n"
        else:
            yield "[å¤±è´¥] è§†é¢‘æœªç”Ÿæˆ\n"
    if stream_output:
        return StreamingResponse(run_and_stream(), media_type="text/plain")
    else:
        try:
            os.makedirs(os.path.dirname(req.save_file), exist_ok=True)
            print("cmd: ", cmd)
            # result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
            )

            # å®æ—¶æ‰“å°å­è¿›ç¨‹æ—¥å¿—
            for line in iter(process.stdout.readline, ''):
                print(line, end="", flush=True)

            process.wait()
            if process.returncode != 0:
                raise HTTPException(status_code=500, detail=f"Video generation failed, code={process.returncode}")

            video_path = f"{req.save_file}.mp4"
            if not os.path.exists(video_path):
                raise HTTPException(status_code=500, detail="Video generation failed. No output file found.")

            return {
                    "cmd": cmd,
                    "message": "Video generated successfully",
                    "video_file": video_path,
                }

        except subprocess.CalledProcessError as e:
            raise HTTPException(
                status_code=500, 
                detail={
                "error": "MultiTalk failed",
                "stdout": e.stdout,
                "stderr": e.stderr,
                "cmd": e.cmd
            }
            )

@app.get("/health")
def health_check():
    return {"status": "ok"}

@app.post("/shutdown")
def shutdown_server():
    """
    è¿œç¨‹å…³é—­ uvicorn æœåŠ¡ + é‡Šæ”¾æ˜¾å­˜
    """
    gc.collect()
    torch.cuda.empty_cache()
    print("ğŸ›‘ æ”¶åˆ°å…³é—­è¯·æ±‚ï¼Œæ­£åœ¨é€€å‡º FastAPI...")
    # å½»åº•é€€å‡º uvicorn
    os.kill(os.getpid(), signal.SIGTERM)
    return {"status": "shutting down"}

@app.get("/busy")
def busy_status():
    return {"busy": getattr(app.state, "busy", False)}
